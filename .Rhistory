theta = rep(0,K)
for (k in 1:K){
beta[k] <- rbeta(1, 1, alpha)
pi[k] = beta[k]*prod(1-beta[1:k-1])
if (G_0 == "normal"){
theta[k] = rnorm(1, 0, 50)
}
}
return( list(pi=pi, theta=theta) )
}
plot_measure <- function(theta, pi) {
plot(theta, pi,
type = "h",              # vertical lines
lwd = 3,
xlab = "theta",
ylab = "pi",
main = "One Realization of Dirichlet Process")
points(theta, pi, pch = 19)
}
# plot a sample of G (sanity check)
pt <- sample_G(n=1, alpha=1, G_0="normal")
plot_measure(pt[["theta"]], pt[["pi"]])
# check that weights sum to approx 1
sum(pt[["pi"]])
dp_mix_gibbs <- function(y,
K       = 10,        # truncation level
n_iter  = 5000,      # total iterations
burn    = 1000,      # burn-in
thin    = 1,
m0      = 0,         # base mean
s0      = 5,         # base sd
sigma   = 1,         # obs sd (known)
alpha   = 1) {       # DP concentration (fixed)
n <- length(y)
s02    <- s0^2
sigma2 <- sigma^2
# Storage
keep_idx <- seq(burn + 1, n_iter, by = thin)
n_keep   <- length(keep_idx)
theta_save <- matrix(NA_real_, n_keep, K)
pi_save    <- matrix(NA_real_, n_keep, K)
z_save     <- matrix(NA_integer_, n_keep, n)
# Initialization
set.seed(NULL)
theta <- rnorm(K, m0, s0)
V     <- c(rbeta(K - 1, 1, alpha), 1)
# compute pi from V
pi <- numeric(K)
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
# random initial allocations
z <- sample(1:K, n, replace = TRUE, prob = pi)
keep_counter <- 0
for (it in 1:n_iter) {
## 1. Update allocations z_i
for (i in 1:n) {
# log-weights for numerical stability
log_w <- log(pi) + dnorm(y[i], mean = theta, sd = sigma, log = TRUE)
log_w <- log_w - max(log_w)
w     <- exp(log_w)
p     <- w / sum(w)
z[i]  <- sample(1:K, size = 1, prob = p)
}
## 2. Update component parameters theta_k
for (k in 1:K) {
idx <- (z == k)
n_k <- sum(idx)
if (n_k > 0) {
y_k  <- y[idx]
s2_k <- 1 / (1 / s02 + n_k / sigma2)
m_k  <- s2_k * (m0 / s02 + sum(y_k) / sigma2)
theta[k] <- rnorm(1, mean = m_k, sd = sqrt(s2_k))
} else {
# draw from base measure
theta[k] <- rnorm(1, m0, s0)
}
}
## 3. Update stick-breaking variables V_k and weights pi_k
n_k <- tabulate(z, nbins = K)  # cluster counts
for (k in 1:(K - 1)) {
a <- 1 + n_k[k]
b <- alpha + sum(n_k[(k + 1):K])
V[k] <- rbeta(1, shape1 = a, shape2 = b)
}
V[K] <- 1
# recompute pi from V
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
## 4. Save draws
if (it %in% keep_idx) {
keep_counter <- keep_counter + 1
theta_save[keep_counter, ] <- theta
pi_save[keep_counter, ]    <- pi
z_save[keep_counter, ]     <- z
}
}
list(theta = theta_save,
pi    = pi_save,
z     = z_save,
K     = K,
iter  = keep_idx)
}
result <- dp_mix_gibbs(y = Y,
K = 30,
n_iter = 5000,
burn = 2000,
thin = 1,
m0 = 0,
s0 = 75,
sigma = 10,
alpha = 1)
theta_mean <- apply(result$theta, 2, mean)
pi_mean <- apply(result$pi, 2, mean)
z_med <- apply(result$z, 2, median)
x_grid <- seq(min(df$Y) - 20, max(df$Y) + 20, length.out = 1000)
dens_df <- map_dfr(1:30, function(k) {
data.frame(
x = x_grid,
y = pi_mean[k] * dnorm(x_grid, mean = theta_mean[k], sd = 10),
component = as.factor(k)
)
})
bw <- (max(df$Y) - min(df$Y)) / 30
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities and original data",
x = "Y", y = "Count") +
theme_minimal()
## Dirichlet Process Gaussian Mixture via Polya-urn Gibbs sampler
## Model:
##  G ~ DP(alpha, N(m0, s0^2))
##  theta_i | G ~ G
##  y_i | theta_i ~ N(theta_i, sigma^2)
dp_gibbs <- function(y, n_iter = 2000,
alpha = 1,
m0 = 0, s0 = 1,       # G0 = N(m0, s0^2)
sigma = 1,            # known obs. sd
verbose = TRUE) {
n <- length(y)
## Storage
# cluster labels for each iteration (optional, can be large)
c_store   <- matrix(NA_integer_, nrow = n_iter, ncol = n)
# number of clusters each iteration
K_store   <- integer(n_iter)
# cluster means; store as list because K varies
phi_store <- vector("list", n_iter)
## --- Initialization ---
# Start with 1 cluster, all points in it
K   <- 1L
c   <- rep(1L, n)
phi <- rnorm(1, mean = m0, sd = s0)  # cluster mean from prior
if (verbose) cat("Starting Gibbs sampler with", n, "points\n")
## --- Gibbs sampling ---
for (it in 1:n_iter) {
## 1. Update labels c_i using Polya-urn full conditional
for (i in 1:n) {
# Remove y_i from its current cluster
k_old <- c[i]
# Counts per cluster BEFORE removing i
n_k <- tabulate(c, nbins = K)
n_k[k_old] <- n_k[k_old] - 1
# If old cluster becomes empty, remove it
if (n_k[k_old] == 0) {
# Drop the empty cluster's mean
phi <- phi[-k_old]
# Decrease number of clusters
K <- K - 1L
# Relabel clusters > k_old to shift down by 1
c[c > k_old] <- c[c > k_old] - 1L
# Recompute counts after relabeling
n_k <- tabulate(c[-i], nbins = K)  # exclude i
}
# Now compute weights for existing clusters
# n_k counts exclude i by construction
w_existing <- numeric(K)
for (k in 1:K) {
if (n_k[k] > 0) {
w_existing[k] <- n_k[k] * dnorm(y[i], mean = phi[k], sd = sigma)
} else {
w_existing[k] <- 0
}
}
# Weight for a new cluster: alpha * prior predictive
# prior predictive: y ~ N(m0, s0^2 + sigma^2)
w_new <- alpha * dnorm(y[i], mean = m0, sd = sqrt(s0^2 + sigma^2))
# Combine and normalize
weights <- c(w_existing, w_new)
if (all(weights == 0)) {
# Numerical guard: if all zero, set uniform
weights <- rep(1, length(weights))
}
probs <- weights / sum(weights)
# Sample new cluster assignment
k_new <- sample.int(K + 1L, size = 1, prob = probs)
if (k_new == K + 1L) {
# New cluster: sample new phi from prior
phi_new <- rnorm(1, mean = m0, sd = s0)
phi <- c(phi, phi_new)
K <- K + 1L
c[i] <- K
} else {
# Existing cluster
c[i] <- k_new
}
} # end loop over i
## 2. Update cluster means phi_k via conjugate Normal-Normal
# Recompute counts and update each occupied cluster
K <- max(c)  # ensure K is consistent
n_k <- tabulate(c, nbins = K)
for (k in 1:K) {
idx_k <- which(c == k)
n_k   <- length(idx_k)
y_k   <- y[idx_k]
# posterior variance s_k^2 and mean m_k
# prior: phi_k ~ N(m0, s0^2)
# likelihood: y_i | phi_k ~ N(phi_k, sigma^2)
prec_post <- 1 / s0^2 + n_k / sigma^2
s_k2      <- 1 / prec_post
m_k       <- s_k2 * (m0 / s0^2 + sum(y_k) / sigma^2)
phi[k] <- rnorm(1, mean = m_k, sd = sqrt(s_k2))
}
## Store
c_store[it, ] <- c
K_store[it]   <- K
phi_store[[it]] <- phi
if (verbose && (it %% 100 == 0)) {
cat("Iter:", it, "K =", K, "\n")
}
}
list(
c_store   = c_store,
K_store   = K_store,
phi_store = phi_store,
final_labels = c,
final_phi    = phi,
alpha = alpha,
m0 = m0, s0 = s0, sigma = sigma
)
}
## -------------------------
## Example usage
## -------------------------
## Simulate some data from a 2-component Gaussian mixture
set.seed(123)
n  <- 200
y  <- c(
rnorm(n/2, mean = -2, sd = 0.5),
rnorm(n/2, mean =  2, sd = 0.5)
)
## Run the DP mixture Gibbs sampler
fit <- dp_gibbs(
Y,
n_iter = 2000,
alpha  = 1,
m0     = 0,
s0     = 75,
sigma  = 10,
verbose = TRUE
)
## Inspect results (e.g., number of clusters over iterations)
plot(fit$K_store, type = "l", xlab = "Iteration", ylab = "Number of clusters")
fit$final_labels
fit$final_phi
K = fit$K_store[length(fit$K_store)]
x_grid <- seq(min(df$Y) - 20, max(df$Y) + 20, length.out = 1000)
dens_df <- map_dfr(1:K, function(k) {
data.frame(
x = x_grid,
y = fit$final_phi[k] * dnorm(x_grid, mean = fit$final_phi[k], sd = 10),
component = as.factor(k)
)
})
bw <- (max(df$Y) - min(df$Y)) / 30
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities and original data",
x = "Y", y = "Count") +
theme_minimal()
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(dens_df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities and original data",
x = "Y", y = "Count") +
theme_minimal()
y * nrow(df) * bw
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(corrplot)
library(gWQS)
library(bkmr)
library(fields)
library(car)
load("exposome.Rdata")
metal_names_df <- codebook |>
filter(family == "Metals") |>
select(variable_name) |>
mutate(variable_name = as.character(variable_name))
metal_names <- metal_names_df$variable_name
# Exposure dataframe for metals only
metals_df <- exposome[,c("ID", metal_names)] |>
mutate(hs_tl_cdich_None = ifelse(hs_tl_cdich_None == "Detected", 1, 0)) |>
mutate(hs_tl_mdich_None = ifelse(hs_tl_mdich_None == "Detected", 1, 0))
colnames(metals_df) <- c("ID", paste0("x", as.character(1:20)))
head(metals_df)
# Full dataframe including covariates + outcomes
full_data <- metals_df |>
left_join(covariates, by = "ID") |>
left_join(phenotype, by = "ID")
head(full_data)
head(exposome)
corrplot(cor(metals_df[2:21]))
# standardizing
full_data[paste0("x", 1:20)] <- scale(full_data[paste0("x", 1:20)])
# putting some of your code up here!
mix_name = paste0("x", as.character(1:20)) # x1, ..., x20
# get covariate names
cov_names_df <- codebook |>
filter(family == "Covariates") |>
select(variable_name) |>
mutate(variable_name = as.character(variable_name))
cov_names <- cov_names_df$variable_name
# From Brent's slides: Adjusted for covariates: Cohort, gestational age at birth, maternal BMI, maternal weight gain, maternal education, parity, native to country of birth
cov_brent <- c(
"h_cohort",
"e3_gac_None",
"h_mbmi_None",
"hs_wgtgain_None",
"h_edumc_None",
"h_parity_None",
"h_native_None"
)
# linear regression
lm_formula <- as.formula(
paste(
"hs_zbmi_who ~",
paste(c(mix_name, cov_brent), collapse = " + ")
)
)
lmall <- lm(lm_formula, data = full_data)
summary(lmall)
vif(lmall)
mix_name = paste0("x", as.character(1:20)) # x1, ..., x20
formula = as.formula(paste(
"hs_zbmi_who ~ pwqs + nwqs + ",
paste(cov_brent, collapse = " + "), collapse = " "))
formula
typeof(formula)
?as.formula
formula = formula(paste(
"hs_zbmi_who ~ pwqs + nwqs + ",
paste(cov_brent, collapse = " + "), collapse = " "))
formula
gwqs_res <- gwqs(formula,
mix_name = mix_name,
data = full_data,
q = 4, # quartiles
validation = 0.6,
b = 500,
family = "gaussian",
seed = 245)
summary(gwqs_res)
# bar plot
gwqs_barplot(gwqs_res)
# scatter plot y vs wqs
gwqs_scatterplot(gwqs_res)
# bar plot
gwqs_barplot(gwqs_res)
# scatter plot residuals vs fitted values
gwqs_fitted_vs_resid(gwqs_res)
# boxplot of the weights estimated at each repeated holdout step
gwqs_boxplot(gwqs_res)
# scatter plot y vs wqs
gwqs_scatterplot(gwqs_res)
?gwqs_scatterplot
gwqs_res
summary(gwqs_res)
summary(gwqs_res)
# bar plot
gwqs_barplot(gwqs_res)]
# bar plot
gwqs_barplot(gwqs_res)
library(broom)
tidy(lmall)
tidy(lmall)
is.element(term, paste0("x", as.character(1:20)))
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20))))
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
kbl(caption = "Mixture Regression Results",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
library(kableExtra)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
kbl(caption = "Mixture Regression Results",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
?tidy
summary(lmall)
metal_names
cor(metals_df$x1, metals_df$x11)
cor(metals_df$x8, metals_df$x6)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`Pr(>|t|)` <0.05)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20))))
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05)
?sort
?arrange
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05) |>
arrange(estimate)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05) |>
arrange(|estimate|)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05) |>
arrange(desc(estimate))
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05) |>
arrange(desc(abs(estimate))) |>
kbl(caption = "Siginificant Mixture Estimates",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
vif(lmall)
summary(gwqs_res)
summary(gwqs_res)
# plots
# bar plot
gwqs_barplot(gwqs_res)
# correlation plot
corrplot(cor(metals_df[2:21]))
cor(metals_df$x1, metals_df$x11)
metal_names
vif(lmall)
?vif
# correlation plot
corrplot(cor(metals_df[2:21]))
cor(metals_df$x1, metals_df$x11)
# standardizing
full_data[paste0("x", 1:20)] <- scale(full_data[paste0("x", 1:20)])
# putting some of your code up here!
mix_name = paste0("x", as.character(1:20)) # x1, ..., x20
# get covariate names
cov_names_df <- codebook |>
filter(family == "Covariates") |>
select(variable_name) |>
mutate(variable_name = as.character(variable_name))
cov_names <- cov_names_df$variable_name
# From Brent's slides: Adjusted for covariates: Cohort, gestational age at birth, maternal BMI, maternal weight gain, maternal education, parity, native to country of birth
cov_brent <- c(
"h_cohort",
"e3_gac_None",
"h_mbmi_None",
"hs_wgtgain_None",
"h_edumc_None",
"h_parity_None",
"h_native_None"
)
# linear regression
lm_formula <- as.formula(
paste(
"hs_zbmi_who ~",
paste(c(mix_name, cov_brent), collapse = " + ")
)
)
lmall <- lm(lm_formula, data = full_data)
summary(lmall)
vif(lmall)
tidy(lmall) |>
filter(is.element(term, paste0("x", as.character(1:20)))) |>
filter(`p.value` <0.05) |>
arrange(desc(abs(estimate))) |>
kbl(caption = "Siginificant Mixture Estimates",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
summary(gwqs_res)
# plots
# bar plot
gwqs_barplot(gwqs_res)
setwd("~/Documents/GitHub/Pop_Uncertainty")
