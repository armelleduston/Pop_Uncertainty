digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
fit_bin$var_comp|>
kbl(caption = "Binomial Variance Parameter Table",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
fit_bin$AIC
head(fit_bin$ranef)
# simulate
sim_bin <- simulate_glmm_data(
K              = 200,
n_per_cluster  = 7,
family         = "binomial",
seed           = 1729
)
### 3(a) ######################################################################
b_pois <- function(x) { exp(x) }
c_pois <- function(y) { -lfactorial(y) }
b_bern  <- function(x) log1p(exp(x))   # log(1 + e^x), stable
p_fun   <- function(x) plogis(x)
b_bern2 <- function(x){
p <- p_fun(x); p * (1 - p)
}
b_bern3 <- function(x){
p <- p_fun(x); p * (1 - p) * (1 - 2 * p)
}
b_bern4 <- function(x){
p <- p_fun(x); p * (1 - p) * (1 - 6*p + 6*p^2)
}
b_bern6 <- function(x){
p <- p_fun(x); p * (1 - p) * (1 - 30*p + 150*p^2 - 240*p^3 + 120*p^4)
}
h_fun <- function(gam, Y, X, Z, theta, family){
n = length(Y)
p = ncol(X)
q = ncol(Z)
beta = cbind(theta[1:p])
alpha = c(theta[-(1:p)])
G = diag(exp(alpha))
eta = X %*% beta + Z %*% gam
ones = rep(1, n)
if (family == "binomial"){
C = -(1/2)*log((2*pi)^q * det(G))
out = t(Y)%*%eta - t(ones)%*%b_bern(eta) - (1/2) * t(gam)%*%chol2inv(chol(G))%*%gam + C
}
if (family == "poisson"){
C = -(1/2)*log((2*pi)^q * det(G)) + t(ones)%*%c_pois(Y)
out = t(Y)%*%eta - t(ones)%*%b_pois(eta) - (1/2) * t(gam)%*%chol2inv(chol(G))%*%gam + C
}
return (as.numeric(out))
}
obj <- function(gam, Y, X, Z, theta, family){
-h_fun(gam, Y, X, Z, theta, family)
}
solve_gamma_mode <- function(Y, X, Z, theta, family) {
# optimize h(gamma_k; theta, D_k)
gamma_start <- cbind(rep(0, ncol(Z)))
gamma_tild <- optim(par = gamma_start,
fn = obj,
method = "BFGS",
Y=Y,
X=X,
Z=Z,
theta=theta,
family=family,
control = list(abstol = 1e-8, maxit=200))
gamma_tild$par
}
# simulate
sim_bin <- simulate_glmm_data(
K              = 200,
n_per_cluster  = 7,
family         = "binomial",
seed           = 1729
)
# fit model
fit_bin <- millipede(
formula.fe = y ~ x1 + x2 + x3 + x4 + x5,
formula.re = ~ 1 + x1 + x2 + x3 + x4 + x5,
id = sim_bin$data$id,
family = "binomial",
data = sim_bin$data
)
saveRDS(fit_bin, bin_path)
print("Binomial Results")
fit_bin$coef |>
kbl(caption = "Binomial Fixed Effects Table",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
fit_bin$var_comp|>
kbl(caption = "Binomial Variance Parameter Table",
booktabs = TRUE,
digits = 3) |>
kable_styling(full_width = FALSE,
latex_options = c("hold_position", "striped"))
fit_bin$AIC
head(fit_bin$ranef)
?optim
knitr::opts_chunk$set(echo = TRUE)
theta_mean <- apply(result$theta, 2, mean)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(mvtnorm)
library(extraDistr)
set.seed(249)
# Truth: 5 clusters,
sigma = 10
thetas = rnorm(5, 0, 75)
Y = rmvnorm(30, thetas, diag(sigma^2, nrow = 5))
df
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
dp_mix_gibbs <- function(y,
K       = 10,        # truncation level
n_iter  = 5000,      # total iterations
burn    = 1000,      # burn-in
thin    = 1,
m0      = 0,         # base mean
s0      = 5,         # base sd
sigma   = 1,         # obs sd (known)
alpha   = 1) {       # DP concentration (fixed)
n <- length(y)
s02    <- s0^2
sigma2 <- sigma^2
# Storage
keep_idx <- seq(burn + 1, n_iter, by = thin)
n_keep   <- length(keep_idx)
theta_save <- matrix(NA_real_, n_keep, K)
pi_save    <- matrix(NA_real_, n_keep, K)
z_save     <- matrix(NA_integer_, n_keep, n)
# Initialization
set.seed(NULL)
theta <- rnorm(K, m0, s0)
V     <- c(rbeta(K - 1, 1, alpha), 1)
# compute pi from V
pi <- numeric(K)
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
# random initial allocations
z <- sample(1:K, n, replace = TRUE, prob = pi)
keep_counter <- 0
for (it in 1:n_iter) {
## 1. Update allocations z_i
for (i in 1:n) {
# log-weights for numerical stability
log_w <- log(pi) + dnorm(y[i], mean = theta, sd = sigma, log = TRUE)
log_w <- log_w - max(log_w)
w     <- exp(log_w)
p     <- w / sum(w)
z[i]  <- sample(1:K, size = 1, prob = p)
}
## 2. Update component parameters theta_k
for (k in 1:K) {
idx <- (z == k)
n_k <- sum(idx)
if (n_k > 0) {
y_k  <- y[idx]
s2_k <- 1 / (1 / s02 + n_k / sigma2)
m_k  <- s2_k * (m0 / s02 + sum(y_k) / sigma2)
theta[k] <- rnorm(1, mean = m_k, sd = sqrt(s2_k))
} else {
# draw from base measure
theta[k] <- rnorm(1, m0, s0)
}
}
## 3. Update stick-breaking variables V_k and weights pi_k
n_k <- tabulate(z, nbins = K)  # cluster counts
for (k in 1:(K - 1)) {
a <- 1 + n_k[k]
b <- alpha + sum(n_k[(k + 1):K])
V[k] <- rbeta(1, shape1 = a, shape2 = b)
}
V[K] <- 1
# recompute pi from V
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
## 4. Save draws
if (it %in% keep_idx) {
keep_counter <- keep_counter + 1
theta_save[keep_counter, ] <- theta
pi_save[keep_counter, ]    <- pi
z_save[keep_counter, ]     <- z
}
}
list(theta = theta_save,
pi    = pi_save,
z     = z_save,
K     = K,
iter  = keep_idx)
}
result <- dp_mix_gibbs(y = Y,
K = 30,
n_iter = 3000,
burn = 1000,
thin = 1,
m0 = 0,
s0 = 75,
sigma = 10,
alpha = 1)
theta_mean <- apply(result$theta, 2, mean)
pi_mean <- apply(result$pi, 2, mean)
z_med <- apply(result$z, 2, median)
length(theta_mean)
length(pi_mean)
result <- dp_mix_gibbs(y = Y,
K = 30,
n_iter = 5000,
burn = 2000,
thin = 1,
m0 = 0,
s0 = 75,
sigma = 10,
alpha = 1)
theta_mean <- apply(result$theta, 2, mean)
pi_mean <- apply(result$pi, 2, mean)
z_med <- apply(result$z, 2, median)
length(theta_mean)
length(pi_mean)
pi_mean
library(dplyr)
library(ggplot2)
library(kableExtra)
library(mvtnorm)
library(extraDistr)
set.seed(249)
# Truth: 5 clusters,
sigma = 10
thetas = rnorm(5, 0, 75)
Y = rmvnorm(30, thetas, diag(sigma^2, nrow = 5))
df
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
dplyr::pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mvtnorm)
library(extraDistr)
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
dplyr::pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
set.seed(249)
# Truth: 5 clusters,
sigma = 10
thetas = rnorm(5, 0, 75)
Y = rmvnorm(30, thetas, diag(sigma^2, nrow = 5))
df
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
df |>
ggplot(aes(x = Y, fill = Cluster)) +
geom_histogram()
theta_mean <- apply(result$theta, 2, mean)
pi_mean <- apply(result$pi, 2, mean)
z_med <- apply(result$z, 2, median)
x_grid <- seq(min(df$Y) - 20, max(df$Y) + 20, length.out = 1000)
dens_df <- map_dfr(1:30, function(k) {
data.frame(
x = x_grid,
y = pi_mean[k] * dnorm(x_grid, mean = theta_mean[k], sd = 10),
component = as.factor(k)
)
})
ggplot() +
geom_histogram(data = df, aes(x = Y, y = after_stat(density)),
fill = "lightgray", color = "white", bins = 30) +
geom_line(data = dens_df, aes(x = x, y = y, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Histogram with Weighted Component Densities",
x = "Y", y = "Density") +
theme_minimal()
ggplot() +
geom_histogram(data = df, aes(x = Y, y = after_stat(density), fill = Cluster), color = "white", bins = 30) +
geom_line(data = dens_df, aes(x = x, y = y, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Histogram with Weighted Component Densities",
x = "Y", y = "Density") +
theme_minimal()
ggplot() +
geom_histogram(data = df, aes(x = Y, y = after_stat(density)),
fill = "lightgray", color = "white", bins = 30) +
geom_line(data = dens_df, aes(x = x, y = y, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Histogram with Weighted Component Densities",
x = "Y", y = "Density") +
theme_minimal()
?after_stat
bw <- (max(df$Y) - min(df$Y)) / 30
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Histogram with Weighted Component Densities",
x = "Y", y = "Count") +
theme_minimal()
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities overlayed onto data",
x = "Y", y = "Count") +
theme_minimal()
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities and original data",
x = "Y", y = "Count") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(mvtnorm)
library(extraDistr)
set.seed(249)
# Truth: 5 clusters,
sigma = 10
thetas = rnorm(5, 0, 75)
Y = rmvnorm(30, thetas, diag(sigma^2, nrow = 5))
df
df <- data.frame(Y) |>
rename_with(function(x) { substr(x, 2, nchar(x)) }) |>
pivot_longer(cols = as.character(1:5),
names_to = "Cluster",
values_to = "Y")
df |>
ggplot(aes(x = Y, fill = Cluster)) +
geom_histogram()
sample_G <- function(n, alpha, G_0){
K=30
beta = rep(0,K)
pi = rep(0,K)
theta = rep(0,K)
for (k in 1:K){
beta[k] <- rbeta(1, 1, alpha)
pi[k] = beta[k]*prod(1-beta[1:k-1])
if (G_0 == "normal"){
theta[k] = rnorm(1, 0, 50)
}
}
return( list(pi=pi, theta=theta) )
}
plot_measure <- function(theta, pi) {
plot(theta, pi,
type = "h",              # vertical lines
lwd = 3,
xlab = "theta",
ylab = "pi",
main = "One Realization of Dirichlet Process")
points(theta, pi, pch = 19)
}
# plot a sample of G (sanity check)
pt <- sample_G(n=1, alpha=1, G_0="normal")
plot_measure(pt[["theta"]], pt[["pi"]])
# check that weights sum to approx 1
sum(pt[["pi"]])
dp_mix_gibbs <- function(y,
K       = 10,        # truncation level
n_iter  = 5000,      # total iterations
burn    = 1000,      # burn-in
thin    = 1,
m0      = 0,         # base mean
s0      = 5,         # base sd
sigma   = 1,         # obs sd (known)
alpha   = 1) {       # DP concentration (fixed)
n <- length(y)
s02    <- s0^2
sigma2 <- sigma^2
# Storage
keep_idx <- seq(burn + 1, n_iter, by = thin)
n_keep   <- length(keep_idx)
theta_save <- matrix(NA_real_, n_keep, K)
pi_save    <- matrix(NA_real_, n_keep, K)
z_save     <- matrix(NA_integer_, n_keep, n)
# Initialization
set.seed(NULL)
theta <- rnorm(K, m0, s0)
V     <- c(rbeta(K - 1, 1, alpha), 1)
# compute pi from V
pi <- numeric(K)
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
# random initial allocations
z <- sample(1:K, n, replace = TRUE, prob = pi)
keep_counter <- 0
for (it in 1:n_iter) {
## 1. Update allocations z_i
for (i in 1:n) {
# log-weights for numerical stability
log_w <- log(pi) + dnorm(y[i], mean = theta, sd = sigma, log = TRUE)
log_w <- log_w - max(log_w)
w     <- exp(log_w)
p     <- w / sum(w)
z[i]  <- sample(1:K, size = 1, prob = p)
}
## 2. Update component parameters theta_k
for (k in 1:K) {
idx <- (z == k)
n_k <- sum(idx)
if (n_k > 0) {
y_k  <- y[idx]
s2_k <- 1 / (1 / s02 + n_k / sigma2)
m_k  <- s2_k * (m0 / s02 + sum(y_k) / sigma2)
theta[k] <- rnorm(1, mean = m_k, sd = sqrt(s2_k))
} else {
# draw from base measure
theta[k] <- rnorm(1, m0, s0)
}
}
## 3. Update stick-breaking variables V_k and weights pi_k
n_k <- tabulate(z, nbins = K)  # cluster counts
for (k in 1:(K - 1)) {
a <- 1 + n_k[k]
b <- alpha + sum(n_k[(k + 1):K])
V[k] <- rbeta(1, shape1 = a, shape2 = b)
}
V[K] <- 1
# recompute pi from V
rem <- 1
for (k in 1:K) {
pi[k] <- V[k] * rem
rem   <- rem * (1 - V[k])
}
## 4. Save draws
if (it %in% keep_idx) {
keep_counter <- keep_counter + 1
theta_save[keep_counter, ] <- theta
pi_save[keep_counter, ]    <- pi
z_save[keep_counter, ]     <- z
}
}
list(theta = theta_save,
pi    = pi_save,
z     = z_save,
K     = K,
iter  = keep_idx)
}
result <- dp_mix_gibbs(y = Y,
K = 30,
n_iter = 5000,
burn = 2000,
thin = 1,
m0 = 0,
s0 = 75,
sigma = 10,
alpha = 1)
theta_mean <- apply(result$theta, 2, mean)
pi_mean <- apply(result$pi, 2, mean)
z_med <- apply(result$z, 2, median)
x_grid <- seq(min(df$Y) - 20, max(df$Y) + 20, length.out = 1000)
dens_df <- map_dfr(1:30, function(k) {
data.frame(
x = x_grid,
y = pi_mean[k] * dnorm(x_grid, mean = theta_mean[k], sd = 10),
component = as.factor(k)
)
})
bw <- (max(df$Y) - min(df$Y)) / 30
ggplot() +
geom_histogram(data = df, aes(x = Y, fill = Cluster),
color = "white", binwidth = bw) +
geom_line(data = dens_df, aes(x = x, y = y * nrow(df) * bw, group = component),
color = "blue", alpha = 0.5) +
labs(title = "Predicted densities and original data",
x = "Y", y = "Count") +
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
read("exposome.Rdata")
read_table("exposome.Rdata")
load("exposome.Rdata")
View(codebook)
setwd("~/Documents/GitHub/Pop_Uncertainty/full_sim")
